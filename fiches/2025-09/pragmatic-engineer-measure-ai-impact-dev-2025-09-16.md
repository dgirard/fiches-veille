# pragmatic-engineer-measure-ai-impact-dev-2025-09-16

## Veille
Pragmatic Engineer - Measure AI impact - Developer productivity - Metrics - GitHub Copilot - DX - Engineering efficiency

## Titre Article
HOW TECH COMPANIES MEASURE THE IMPACT OF AI ON SOFTWARE DEVELOPMENT

## Date
2025-09-16

## URL
https://newsletter.pragmaticengineer.com/p/how-tech-companies-measure-the-impact-of-ai?utm_source=tldrnewsletter

## Keywords
AI impact, software development, engineering efficiency, developer productivity, AI tools, metrics, GitHub Copilot, Google, Microsoft, Dropbox, Monzo, Atlassian, DX, AI Measurement Framework, Change Failure Rate, PR throughput, developer experience, CSAT, time savings

## Authors
Gergely Orosz and Laura Tacho

## Ton
**Profil:** Professionnel-Analytique | Co-auteurs expert | Éducative-Prescriptive | Intermédiaire-Expert

Orosz et Tacho adoptent collaborative expert voice combinant reporting et framework building. Data depuis 18 companies (Google, GitHub, Microsoft, Dropbox) grounds recommendations empirically. Structure systematic blend core metrics + AI-specific reveals framework thinking. Citations case studies concrets (Dropbox 90% adoption, Microsoft BDDs, Monzo challenges) illustrate abstract principles. Langage prescriptif ("should blend", "must track", "essential") provides actionable guidance. Warnings explicit (quality risks, maintainability debt, acceptance rate limitations) show intellectual honesty. Vise engineering leaders avec blend strategic thinking et tactical implementation. Typique Pragmatic Engineer deep-dives combining industry research avec practical recommendations.

## Pense-betes
- **18 leading tech companies** étudiées (Google, GitHub, Microsoft, Dropbox, Monzo, Atlassian...)
- **85% engineers use AI tools**, mais manque de metrics claires pour justifier investment
- **Core metrics + AI-specific** : blend existing (CFR, PR Throughput, PR Cycle Time, DevEx) avec nouveaux (adoption rate, CSAT, time saved, AI spend)
- **Dropbox results** : **90% AI adoption**, engineers merge **20% more PRs** avec reduced CFR
- **Segment data** : AI users vs non-AI users, before/after AI, by role/tenure/language
- **Balance Speed with Quality** : track metrics keeping each other in check (PR throughput + CFR)
- **Developer Experience prioritaire** : measure satisfaction et experience, crucial pour sustainable adoption
- **3-layer data collection** : system data + periodic surveys + experience sampling
- **Experimental mindset** : approach measurement avec clear goal, test predictions
- **"Bad developer days" (BDDs)** : Microsoft metric assessing AI impact sur daily toil
- **Acceptance rate waning** : no longer leading metric, fails capture maintainability/bugs
- **Agent telemetry** : emerging area expected evolve significantly
- **Monzo case** : objective measurement challenging (vendor data hoarding), subjective sentiment + specific use cases (code migrations) demonstrate clear value

## RésuméDe400mots

Ce deepdive explore how **18 leading tech companies**, incluant Google, GitHub, Microsoft, et Dropbox, measure impact de AI sur software development, addressing challenge de justifying increasing investment dans AI coding tools. Authored par Gergely Orosz et Laura Tacho (CTO at DX), article highlights que tandis que **85% des engineers use AI tools**, many engineering leaders struggle avec lack de clear metrics pour assess true value beyond superficial measures comme lines of code (LOC).

**Core Message : Blend Metrics**

Core message est qu'effective AI impact measurement requires **blend de existing "core" engineering metrics et new AI-specific ones**. Companies ne doivent pas abandon traditional metrics comme Change Failure Rate, PR Throughput, PR Cycle Time, et Developer Experience, car AI's ultimate goal est enhance ces fundamental aspects de software delivery. Instead, ces core metrics should be tracked conjointement avec AI adoption rates, customer satisfaction (CSAT) de AI tools, time saved per engineer, et AI spend. **Dropbox**, pour instance, achieved **90% AI adoption rate** et saw engineers merge **20% more pull requests** avec reduced change failure rate en combining ces metric types.

**Segmentation et Experimental Mindset**

Crucial aspect est **breaking down metrics by AI-usage level**, comparing AI users avec non-AI users, et analyzing trends over time. Cette "slicing and dicing" by role, tenure, ou programming language helps identify specific groups benefiting most ou requiring further training. Article emphasizes importance d'**experimental mindset**, où data est used pour answer specific questions et test predictions about AI's influence.

**Quality, Maintainability, Developer Experience**

Maintaining vigilance over **code quality, maintainability, et developer experience** est paramount. Authors warn against potential pour AI-assisted development créer "biggest pile of tech debt" si not carefully managed. Tracking metrics keeping each other in check, comme speed alongside quality (PR throughput et CFR), est essential. Beyond system metrics, self-reported data sur "change confidence", "code maintainability", et "perception of quality" vital pour capture long-term impacts system data alone cannot reveal. **Developer experience**, often misunderstood comme superficial perks, highlighted comme critical pour reducing friction dans entire development lifecycle.

**Emerging Trends et Challenges**

Article delves dans unique metrics et emerging trends. Microsoft uses **"bad developer days" (BDDs)** pour assess AI's impact sur daily toil, tandis que Glassdoor measures experimentation outcomes (A/B tests) pour gauge innovation. **"Acceptance rate"** de AI suggestions, once leading metric, waning in popularity due à narrow scope, failing capture maintainability, bug introduction, ou overall developer productivity. **Spend et cost analysis**, currently not widely measured pour avoid discouraging usage, predicted become more scrutinized as AI budgets grow. **Agent telemetry et measurement beyond code authoring** identified comme areas expected evolve significantly.

**AI Measurement Framework et Data Layers**

Article introduces **AI Measurement Framework**, recommended set de metrics blending AI et core engineering metrics, avec developer experience at center. Advocates pour layered approach à data collection, combining quantitative system data (from AI tools, GitHub, JIRA, CI/CD) avec qualitative periodic surveys et in-the-moment experience sampling. **Monzo Bank's experience** shared comme case study, revealing que tandis qu'objective measurement est challenging due à vendor data hoarding, subjective engineer sentiment et specific use cases comme code migrations demonstrate clear value.

## GrapheDeConnaissance

### Triples

| Sujet | Type Sujet | Prédicat | Objet | Type Objet | Confiance | Temporalité | Source |
|-------|-----------|----------|-------|-----------|-----------|-------------|--------|
| Gergely Orosz | PERSONNE | co_publie | AI Measurement Framework | METHODOLOGIE | 0.97 | STATIQUE | déclaré_article |
| Laura Tacho | PERSONNE | co_publie | AI Measurement Framework | METHODOLOGIE | 0.97 | STATIQUE | déclaré_article |
| Laura Tacho | PERSONNE | occupe_le_poste_de | CTO chez DX | ORGANISATION | 0.98 | DYNAMIQUE | déclaré_article |
| DX | ORGANISATION | aide | entreprises à mesurer l'efficacité ingénierie | CONCEPT | 0.95 | DYNAMIQUE | déclaré_article |
| AI Measurement Framework | METHODOLOGIE | recommande | blend métriques core et IA | CONCEPT | 0.96 | STATIQUE | déclaré_article |
| Dropbox | ORGANISATION | atteint | 90% taux d'adoption IA | CONCEPT | 0.98 | STATIQUE | déclaré_article |
| Dropbox | ORGANISATION | observe | augmentation 20% merge de PRs | CONCEPT | 0.95 | STATIQUE | déclaré_article |
| Microsoft | ORGANISATION | utilise | Bad Developer Days | METHODOLOGIE | 0.97 | DYNAMIQUE | déclaré_article |
| Monzo Bank | ORGANISATION | rencontre | difficultés mesure objective IA | CONCEPT | 0.93 | DYNAMIQUE | déclaré_article |
| acceptance rate | CONCEPT | devient_obsolète | mesure de productivité IA | CONCEPT | 0.88 | DYNAMIQUE | déclaré_article |
| LeadDev | ORGANISATION | publie | AI Impact Report 2025 | EVENEMENT | 0.96 | STATIQUE | déclaré_article |
| METR study | EVENEMENT | contredit | perception de gain de vitesse IA | CONCEPT | 0.90 | STATIQUE | déclaré_article |
| LOC | CONCEPT | s_oppose_à | mesure pertinente productivité | CONCEPT | 0.92 | ATEMPOREL | déclaré_article |
| agent telemetry | CONCEPT | prédit | évolution significative mesure IA | CONCEPT | 0.82 | DYNAMIQUE | inféré |

### Entités

| Entité | Type | Attribut | Valeur | Action |
|--------|------|----------|--------|--------|
| Gergely Orosz | PERSONNE | rôle | Auteur newsletter The Pragmatic Engineer | AJOUT |
| Laura Tacho | PERSONNE | rôle | CTO chez DX, co-auteure AI Measurement Framework | AJOUT |
| DX | ORGANISATION | secteur | Mesure productivité ingénierie et expérience développeur | AJOUT |
| AI Measurement Framework | METHODOLOGIE | origine | DX, basé sur données 400+ entreprises | AJOUT |
| Dropbox | ORGANISATION | résultat | 90% adoption IA, +20% PRs fusionnés | AJOUT |
| Microsoft | ORGANISATION | métrique distinctive | Bad Developer Days (BDDs) | AJOUT |
| Monzo Bank | ORGANISATION | défi | Opacité des données vendeurs d'outils IA | AJOUT |
| Change Failure Rate | METHODOLOGIE | catégorie | Métrique core ingénierie logicielle | AJOUT |
| acceptance rate | CONCEPT | statut | Métrique en déclin, portée trop étroite | AJOUT |
| LOC | CONCEPT | évaluation | Mauvaise mesure de productivité développeur | AJOUT |
| agent telemetry | CONCEPT | statut | Domaine émergent de mesure IA | AJOUT |
