# pragmatic-engineer-measure-ai-impact-dev-2025-09-16

## Veille
Pragmatic Engineer - Measure AI impact - Developer productivity - Metrics - GitHub Copilot - DX - Engineering efficiency

## Titre Article
HOW TECH COMPANIES MEASURE THE IMPACT OF AI ON SOFTWARE DEVELOPMENT

## Date
2025-09-16

## URL
https://newsletter.pragmaticengineer.com/p/how-tech-companies-measure-the-impact-of-ai?utm_source=tldrnewsletter

## Keywords
AI impact, software development, engineering efficiency, developer productivity, AI tools, metrics, GitHub Copilot, Google, Microsoft, Dropbox, Monzo, Atlassian, DX, AI Measurement Framework, Change Failure Rate, PR throughput, developer experience, CSAT, time savings

## Authors
Gergely Orosz and Laura Tacho

## Pense-betes
- **18 leading tech companies** étudiées (Google, GitHub, Microsoft, Dropbox, Monzo, Atlassian...)
- **85% engineers use AI tools**, mais manque de metrics claires pour justifier investment
- **Core metrics + AI-specific** : blend existing (CFR, PR Throughput, PR Cycle Time, DevEx) avec nouveaux (adoption rate, CSAT, time saved, AI spend)
- **Dropbox results** : **90% AI adoption**, engineers merge **20% more PRs** avec reduced CFR
- **Segment data** : AI users vs non-AI users, before/after AI, by role/tenure/language
- **Balance Speed with Quality** : track metrics keeping each other in check (PR throughput + CFR)
- **Developer Experience prioritaire** : measure satisfaction et experience, crucial pour sustainable adoption
- **3-layer data collection** : system data + periodic surveys + experience sampling
- **Experimental mindset** : approach measurement avec clear goal, test predictions
- **"Bad developer days" (BDDs)** : Microsoft metric assessing AI impact sur daily toil
- **Acceptance rate waning** : no longer leading metric, fails capture maintainability/bugs
- **Agent telemetry** : emerging area expected evolve significantly
- **Monzo case** : objective measurement challenging (vendor data hoarding), subjective sentiment + specific use cases (code migrations) demonstrate clear value

## RésuméDe400mots

Ce deepdive explore how **18 leading tech companies**, incluant Google, GitHub, Microsoft, et Dropbox, measure impact de AI sur software development, addressing challenge de justifying increasing investment dans AI coding tools. Authored par Gergely Orosz et Laura Tacho (CTO at DX), article highlights que tandis que **85% des engineers use AI tools**, many engineering leaders struggle avec lack de clear metrics pour assess true value beyond superficial measures comme lines of code (LOC).

**Core Message : Blend Metrics**

Core message est qu'effective AI impact measurement requires **blend de existing "core" engineering metrics et new AI-specific ones**. Companies ne doivent pas abandon traditional metrics comme Change Failure Rate, PR Throughput, PR Cycle Time, et Developer Experience, car AI's ultimate goal est enhance ces fundamental aspects de software delivery. Instead, ces core metrics should be tracked conjointement avec AI adoption rates, customer satisfaction (CSAT) de AI tools, time saved per engineer, et AI spend. **Dropbox**, pour instance, achieved **90% AI adoption rate** et saw engineers merge **20% more pull requests** avec reduced change failure rate en combining ces metric types.

**Segmentation et Experimental Mindset**

Crucial aspect est **breaking down metrics by AI-usage level**, comparing AI users avec non-AI users, et analyzing trends over time. Cette "slicing and dicing" by role, tenure, ou programming language helps identify specific groups benefiting most ou requiring further training. Article emphasizes importance d'**experimental mindset**, où data est used pour answer specific questions et test predictions about AI's influence.

**Quality, Maintainability, Developer Experience**

Maintaining vigilance over **code quality, maintainability, et developer experience** est paramount. Authors warn against potential pour AI-assisted development créer "biggest pile of tech debt" si not carefully managed. Tracking metrics keeping each other in check, comme speed alongside quality (PR throughput et CFR), est essential. Beyond system metrics, self-reported data sur "change confidence", "code maintainability", et "perception of quality" vital pour capture long-term impacts system data alone cannot reveal. **Developer experience**, often misunderstood comme superficial perks, highlighted comme critical pour reducing friction dans entire development lifecycle.

**Emerging Trends et Challenges**

Article delves dans unique metrics et emerging trends. Microsoft uses **"bad developer days" (BDDs)** pour assess AI's impact sur daily toil, tandis que Glassdoor measures experimentation outcomes (A/B tests) pour gauge innovation. **"Acceptance rate"** de AI suggestions, once leading metric, waning in popularity due à narrow scope, failing capture maintainability, bug introduction, ou overall developer productivity. **Spend et cost analysis**, currently not widely measured pour avoid discouraging usage, predicted become more scrutinized as AI budgets grow. **Agent telemetry et measurement beyond code authoring** identified comme areas expected evolve significantly.

**AI Measurement Framework et Data Layers**

Article introduces **AI Measurement Framework**, recommended set de metrics blending AI et core engineering metrics, avec developer experience at center. Advocates pour layered approach à data collection, combining quantitative system data (from AI tools, GitHub, JIRA, CI/CD) avec qualitative periodic surveys et in-the-moment experience sampling. **Monzo Bank's experience** shared comme case study, revealing que tandis qu'objective measurement est challenging due à vendor data hoarding, subjective engineer sentiment et specific use cases comme code migrations demonstrate clear value.
